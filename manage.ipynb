{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2399e91-3a1b-4585-a260-e646a98fe001",
   "metadata": {},
   "source": [
    "## parking mangment yolov5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66080372-4cec-4479-889d-223003a2ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 cars, 3 trucks, 133.0ms\n",
      "Speed: 5.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'pandas'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m points \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#each detections is a row \u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas\u001b[49m()\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#print(row)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxmin\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     47\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mymin\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ultralytics\\utils\\__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Results' object has no attribute 'pandas'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "# to get points of roi \n",
    "def POINTS(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_RBUTTONDBLCLK :  \n",
    "        colorsBGR = [x, y]\n",
    "        print(colorsBGR)\n",
    "        \n",
    "\n",
    "cv2.namedWindow('FRAME')\n",
    "#cv2.setMouseCallback('FRAME', POINTS)            \n",
    "    \n",
    "           \n",
    "\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') #use any fourcc type to improve quality for the saved video\n",
    "out = cv2.VideoWriter('videos/output.avi', fourcc, 20.0, (640,480)) #Video settings\n",
    "\n",
    "cap=cv2.VideoCapture('videos/parking.mp4')\n",
    "\n",
    "\n",
    "# roi \n",
    "area = [(26,433),(9,516),(389, 492),(786,419),(720,368)]\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame=cv2.resize(frame,(1020,600))\n",
    "\n",
    "    results=model(frame)\n",
    "    #print(results.xyxy[0])\n",
    "    #print(results.pandas().xyxy[0])\n",
    "    #results.pandas()\n",
    "    points = []\n",
    "    \n",
    "    #each detections is a row \n",
    "    for index, row in results.pandas().xyxy[0].iterrows():\n",
    "        #print(row)\n",
    "        \n",
    "        x1 = int(row['xmin'])\n",
    "        y1 = int(row['ymin'])\n",
    "        x2 = int(row['xmax'])\n",
    "        y2 = int(row['ymax'])\n",
    "        d=(row['name'])\n",
    "        # to get center of detection \n",
    "        cx=int(x1+x2)//2\n",
    "        cy=int(y1+y2)//2\n",
    "        if 'car' in d:\n",
    "            # to check if the point is inside or outside area , output : -1 if the point is not and 1 if the point is in the area \n",
    "            results = cv2.pointPolygonTest(np.array(area,np.int32),(cx,cy),False)\n",
    "            #print(results)\n",
    "            # if the car is in the roi \n",
    "            if results>=0:\n",
    "                # draw rectenagle on car \n",
    "                cv2.rectangle(frame,(x1,y1),(x2,y2),(0,0,255),3)\n",
    "                cv2.putText(frame,str(d),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "                points.append([cx])\n",
    "    # number of cars in roi \n",
    "    a = len(points)\n",
    "    #print(points)\n",
    "    cv2.polylines(frame,[np.array(area,np.int32)],True,(0,255,0),2)\n",
    "    cv2.putText(frame,'number of cars in parking ='+str(a),(50,80),cv2.FONT_HERSHEY_PLAIN,2,(0,0,255),2)\n",
    "                \n",
    "    cv2.imshow(\"FRAME\",frame)\n",
    "    out.write(frame) #save your video\n",
    "    #cv2.setMouseCallback(\"FRAME\",POINTS)\n",
    "   \n",
    "\n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e3000-e577-407b-ae13-27c22f4f312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.waitKey(1)  1  بتعرض المعدل الطبيعي للفريمات "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6577f2f6-c383-4bd8-868d-817e41ce2a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [xmin, ymin, xmax, ymax, confidence, class, name]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c67dfe-71cc-424a-a84e-e76691986765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
